---
title: 评估模型：模块参考
titleSuffix: Azure Machine Learning
description: 了解如何使用 Azure 机器学习中的“评估模型”模块来度量已训练模型的准确度。
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: xiaoharper
ms.author: zhanxia
ms.date: 11/19/2019
ms.openlocfilehash: 72d9dd04ea791f3aa359b5b4727122afd800197b
ms.sourcegitcommit: 623d64ef33e80d5f84b6dcf6d1ef4120fe4b8c08
ms.translationtype: HT
ms.contentlocale: zh-CN
ms.lasthandoff: 01/02/2020
ms.locfileid: "75598695"
---
# <a name="evaluate-model-module"></a>“评估模型”模块

本文介绍 Azure 机器学习设计器（预览版）中的一个模块。

使用此模块可以度量已训练模型的准确度。 提供包含通过模型生成的评分的数据集后，“评估模型”模块将计算一组符合行业标准的评估指标。 
  
 “评估模型”返回的指标取决于评估的模型类型：   
  
-   **分类模型**    
-   **回归模型**    


> [!TIP]
> 如果你不熟悉模型评估，我们建议观看 Stephen Elston 博士制作的视频系列，这也是 EdX 提供的[机器学习课程](https://blogs.technet.microsoft.com/machinelearning/2015/09/08/new-edx-course-data-science-machine-learning-essentials/)的一部分。 


可通过三种方式使用“评估模型”模块： 

+ 针对训练数据生成评分，然后基于这些评分评估模型
+ 在模型中生成评分，但将这些评分与保留的测试集中的评分进行比较
+ 使用相同的数据集比较两个不同但相关的模型的评分

## <a name="use-the-training-data"></a>使用训练数据

若要评估某个模型，必须连接包含一组输入列和评分的数据集。  如果没有其他可用数据，可以使用原始数据集。

1. 将[评分模型](./score-model.md)的“已评分数据集”输出连接到“评估模型”的输入。   
2. 单击“评估模型”模块，并运行管道来生成评分。 

## <a name="use-testing-data"></a>使用测试数据

机器学习的一种常用方案是使用[拆分](./split-data.md)模块或[分区和采样](./partition-and-sample.md)模块，将原始数据集划分成训练和测试数据集。 

1. 将[评分模型](score-model.md)的“已评分数据集”输出连接到“评估模型”的输入。   
2. 将包含测试数据的“拆分数据”模块的输出连接到“评估模型”右侧的输入。 
2. 单击“评估模型”模块，选择“运行所选项”来生成评分。  

## <a name="compare-scores-from-two-models"></a>比较两个模型的评分

还可以将第二组评分连接到“评估模型”。   评分可以是包含已知结果的共享评估集，也可以是不同模型针对相同数据提供的一组结果。

此功能非常有用，因为它可以让你轻松比较两个不同模型针对相同数据提供的结果。 或者，可以比较两个不同运行使用不同参数针对相同数据提供的评分。

1. 将[评分模型](score-model.md)的“已评分数据集”输出连接到“评估模型”的输入。   
2. 将第二个模型的“评分模型”模块的输出连接到“评估模型”右侧的输入。 
3. 右键单击“评估模型”，并选择“运行所选项”来生成评分。  

## <a name="results"></a>结果

运行“评估模型”后，右键单击该模块并选择“评估结果”来查看结果。   方法：

+ 将结果另存为数据集，以方便使用其他工具进行分析
+ 在设计器中生成可视化效果

如果将数据集连接到“评估模型”的两个输入，结果将包含两个数据集或两个模型的指标。 
附加到左侧端口的模型或数据先显示在报告中，其后是附加到右侧端口的数据集或模型的指标。  

例如，下图显示了基于相同数据但不同参数生成的两个聚类模型的结果比较。  

![AML&#95;Comparing2Models](media/module/aml-comparing2models.png "AML_Comparing2Models")  

由于这是一个聚类模型，因此评估结果与比较两个回归模型的评分或比较两个分类模型时出现的结果不同。 不过，提供的结果在总体上是相同的。 

## <a name="metrics"></a>指标

本部分介绍针对支持与“评估模型”配合使用的特定模型类型返回的指标： 

+ [分类模型](#bkmk_classification)
+ [回归模型](#bkmk_regression)

###  <a name="bkmk_classification"></a> 分类模型的指标

评估分类模型时，将报告以下指标。 如果比较模型，将按选择用于评估的指标为这些模型排名。  
  
-   “准确度”以真实结果数与案例总数之比的形式度量分类模型的好坏。   
  
-   “精准率”是真实结果与所有正面结果之比。   
  
-   “召回率”是模型返回的所有正确结果的小数。   
  
-   “F 评分”计算为精准率与召回率的加权平均值，介于 0 和 1 之间，理想的 F 评分值为 1。   
  
-   “AUC”度量绘制的曲线下面的面积（在 y 轴上绘制真报率，在 x 轴上绘制误报率）。  此指标非常有用，因为它提供单个数字让你比较不同类型的模型。  
  
- “平均对数损失”是用于表示错误结果的惩罚的单个评分。  它计算为以下两个概率分布之差 – 真实分布，以及模型中的分布。  
  
- “训练对数损失”是表示分类器相比随机预测的优势的单个评分。  对数损失通过将模型输出的概率与标签中的已知值（真实值）进行比较，来度量模型的不确定性。 我们希望最大程度地减小整个模型的对数损失。

##  <a name="bkmk_regression"></a> 回归模型的指标
 
针对回归模型返回的指标旨在估计误差量。  如果观测值与预测值之间的差很小，则认为模型能够很好地拟合数据。 不过，查看残差模式（任何一个预测点与其对应实际值之间的差）可以很好地判断模型中的潜在偏差。  
  
 将报告以下指标来评估回归模型。 比较模型时，将按选择用于评估的指标为这些模型排名。  
  
- “平均绝对误差 (MAE)”度量预测结果与实际结果的接近程度；因此，评分越低越好。   
  
- “均方根误差 (RMSE)”创建单个值用于汇总模型中的误差。  求差的平方时，指标将忽略过预测与欠预测之差。  
  
- “相对绝对误差 (RAE)”是预期值与实际值之间的相对绝对差；之所以是相对的，是因为平均差将除以算术平均值。   
  
- 类似地，“相对平方误差 (RSE)”除以实际值的总平方误差，以此规范化预测值的总平方误差。   
  
- “平均 0-1 误差 (MZOE)”指示预测是否正确。   换言之：当 `x!=y` 时，`ZeroOneLoss(x,y) = 1`；否则 `0`。
  
- “决定系数”（通常称为 R<sup>2</sup>）表示模型的预测能力，值为 0 到 1。  如果为 0，则模型是随机的（不解释任何信息）；1 表示完美拟合。 不过，在解释 R<sup>2</sup> 值时请小心，因为低值可能完全正常，而高值可能是可疑的。
  

## <a name="next-steps"></a>后续步骤

参阅 Azure 机器学习[可用的模块集](module-reference.md)。 